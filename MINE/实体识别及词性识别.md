#### 提取实体

extract_ner.py

```python
# encoding=utf8
import nltk, json

from busi_analy.config.nlp_conf import stanford_nlp
from busi_analy.config.tools import ner_stanford

def get_stanford_ner_nodes(parent):
    # type(parent)= tree
    # like：Tree('S', [Tree('DATE', [('2018年', 'DATE')]), ('，', 'O'), Tree('PERSON', [('金正恩', 'PERSON')]), ('来到', 'O'), Tree('LOCATION', [('美国', 'COUNTRY'), ('洛杉矶', 'CITY')]), ('，', 'O'), ('他', 'O'), ('有', 'O'), ('80％', 'NUMBER'), ('的', 'O'), ('概率', 'O'), ('能够', 'O'), ('进球', 'O'), ('，', 'O'), ('花', 'O'), ('了', 'O'), ('1000', 'MONEY'), ('元', 'MONEY'), ('，', 'O'), ('50', 'MONEY'), ('美元', 'MONEY'), ('，', 'O'), ('在', 'O'), Tree('ORGANIZATIONL', [('斯坦福', 'ORGANIZATION'), ('大学', 'ORGANIZATION')]), ('开展', 'O'), ('了', 'O'), ('旅游', 'O'), ('会', 'O'), ('，', 'O'), Tree('DATE', [('20:15', 'TIME')]), ('开', 'O'), ('完', 'O'), ('会议', 'O'), ('。', 'O')])
    # 树的节点需要定义规则
    date = ''
    name = ''
    org = ''
    loc = ''
    num = ''
    mon = ''
    for node in parent:
        if type(node) is nltk.Tree:  # nltk.Tree = <class 'nltk.tree.Tree'>
            if node.label() == 'DATE':
                date = date + " " + ''.join([i[0] for i in node])
            elif node.label() == 'PERSON':
                name = name + " " + ''.join([i[0] for i in node])
            elif node.label() == 'ORGANIZATIONL':
                org = org + " " + ''.join([i[0] for i in node])
            elif node.label() == 'LOCATION':
                loc = loc + " " + ''.join([i[0] for i in node])
            elif node.label() == 'NUMBER':
                num = num + " " + ''.join([i[0] for i in node])
            elif node.label() == 'MONEY':
                mon = mon + " " + ''.join([i[0] for i in node])
    if len(name) > 0 or len(date) > 0 or len(org) > 0 or len(loc) > 0 or len(num) > 0 or len(mon) > 0:
        return {'date': date, 'name': name, 'org': org, 'loc': loc, 'num': num, 'mon': mon}
    else:
        return {}


def grammer_parse(raw_sentence=None, file_object=None):
    # assert grammer_type in set(['hanlp_keep','stanford_ner_drop','stanford_pos_drop'])
    if len(raw_sentence.strip()) < 1:
        return False
    grammer_dict = \
        {
            'stanford_ner_drop': r"""
            DATE:
            {<DATE>+<MISC>?<DATE>*}
            {<DATE>+}
            {<TIME>+}
            PERSON:{<PERSON>+}
            ORGANIZATIONL:{<ORGANIZATION>+}
            LOCATION:{<LOCATION|STATE_OR_PROVINCE|CITY|COUNTRY>+}  
            NUMBER:{<PERCENT>+<MISC>?<PERCENT>*}
            {<PERCENT>+}
            {<NUMBER>+}
            MONEY:{<MONEY>+}
        """,
            'hanlp_keep': r"""
            NP: 
            {<m|mg|Mg|mq|q|qg|qt|qv|s|>*<a|an|ag>*<s|g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|o|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+<f>?<ude1>?<g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|o|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+}
            {<n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+<cc>+<n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+}
            {<m|mg|Mg|mq|q|qg|qt|qv|s|>*<q|qg|qt|qv>*<f|b>*<vi|v|vn|vg|vd>+<ude1>+<n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+}
            {<g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+<vi>?}
            VP:{<v|vd|vg|vf|vl|vshi|vyou|vx|vi|vn>+}
        """,
        }

    stanford_ner_drop_rp = nltk.RegexpParser(grammer_dict['stanford_ner_drop'])
    # <chunk.RegexpParser with 4 stages>
    try:
        stanford_ner_drop_result = stanford_ner_drop_rp.parse(ner_stanford(raw_sentence))
        # like：Tree('S', [Tree('DATE', [('2018年', 'DATE')]), ('，', 'O'), Tree('PERSON', [('金正恩', 'PERSON')]), ('来到', 'O'), Tree('LOCATION', [('美国', 'COUNTRY'), ('洛杉矶', 'CITY')]), ('，', 'O'), ('他', 'O'), ('有', 'O'), ('80％', 'NUMBER'), ('的', 'O'), ('概率', 'O'), ('能够', 'O'), ('进球', 'O'), ('，', 'O'), ('花', 'O'), ('了', 'O'), ('1000', 'MONEY'), ('元', 'MONEY'), ('，', 'O'), ('50', 'MONEY'), ('美元', 'MONEY'), ('，', 'O'), ('在', 'O'), Tree('ORGANIZATIONL', [('斯坦福', 'ORGANIZATION'), ('大学', 'ORGANIZATION')]), ('开展', 'O'), ('了', 'O'), ('旅游', 'O'), ('会', 'O'), ('，', 'O'), Tree('DATE', [('20:15', 'TIME')]), ('开', 'O'), ('完', 'O'), ('会议', 'O'), ('。', 'O')])
        # tyep= <class 'nltk.tree.Tree'>
        # 有在grammer_dict定义才能形成Tree节点
        # stanford_ner_drop_result.draw() # 绘制树状图
    except:
        print("the error sentence is {}".format(raw_sentence))
    else:
        # 调用get_stanford_ner_nodes 函数，返回{'date':date,'name':name,'org':org,'loc':loc}
        stanford_keep_drop_dict = get_stanford_ner_nodes(stanford_ner_drop_result)
        if len(stanford_keep_drop_dict) > 0:
            # json格式写入文件
            file_object.write(json.dumps(stanford_keep_drop_dict, skipkeys=False,
                                         ensure_ascii=False,
                                         check_circular=True,
                                         allow_nan=True,
                                         cls=None,
                                         indent=4,  # 缩进，美观
                                         separators=None,
                                         default=None,
                                         sort_keys=False))

if __name__ == '__main__':
    import time
    print('-------测试开始--------')
    t1=time.time()
    fp = open("../files/t1_ner.txt", 'r', encoding='utf-8')
    fout = open("../files/out_4.txt", 'w', encoding='utf-8')
    [grammer_parse(line.strip(), fout) for line in fp if len(line.strip()) > 0]
    print(time.time()-t1)
    fout.close()
    stanford_nlp.close()

```



#### 提取动名词

extract_nvp.py

```python
#encoding=utf8
import nltk,re

from busi_analy.config.nlp_conf import huanhang, keep_pos_p, merge_pos
from busi_analy.config.tools import cut_hanlp_1

def getNodes(parent,model_tagged_file):
    text=''
    for node in parent:
        if type(node) is nltk.Tree:
            if node.label() == 'NP':   
                text+=''.join(node_child[0].strip() for node_child in node.leaves())+"/NP"+3*" "
            if node.label() == 'VP':
                text+=''.join(node_child[0].strip() for node_child in node.leaves())+"/VP"+3*" "
        else:
            if node[1] in keep_pos_p:
                text+=node[0].strip()+"/PP"+3*" "  
            if node[0] in huanhang : 
                text+=node[0].strip()+"/O"+3*" "                    
            if node[1] not in merge_pos:
                text+=node[0].strip()+"/O"+3*" "                             
            #print("hh")
    # 提取非 "/O" 类型的数据
    t = [i for i in text.split(3 * " ") if not re.search('/O', i)]
    print(t)
    # 读入文件
    model_tagged_file.write(text+"\n")     

def grammer(sentence,model_tagged_file):#{内/f 训/v 师/ng 单/b 柜/ng}
    """
    input sentences shape like :[('工作', 'vn'), ('描述', 'v'), ('：', 'w'), ('我', 'rr'), ('曾', 'd'), ('在', 'p')]
    """
    grammar1 = r"""
        NP: 
        {<m|mg|Mg|mq|q|qg|qt|qv|s|>*<a|an|ag>*<s|g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|o|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+<f>?<ude1>?<g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|o|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+}
        {<n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+<cc>+<n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+}
        {<m|mg|Mg|mq|q|qg|qt|qv|s|>*<q|qg|qt|qv>*<f|b>*<vi|v|vn|vg|vd>+<ude1>+<n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+}
        {<g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+<vi>?}
        VP:{<v|vd|vg|vf|vl|vshi|vyou|vx|vi|vn>+}
        """
    cp = nltk.RegexpParser(grammar1)
    try :
        result = cp.parse(sentence) 
    except:
        pass
    else:
        getNodes(result,model_tagged_file)

def data_read():
    fout=open('../files/sx5_nvp.txt','w',encoding='utf8')
    for line in open('../files/sx_5.txt','r',encoding='utf8'):
        line=line.strip()
        grammer(cut_hanlp_1(line),fout)
    fout.close()

if __name__=='__main__':
    print('--------测试开始--------')
    data_read()

```



备注：文件格式（多行文本，一行一句。）









