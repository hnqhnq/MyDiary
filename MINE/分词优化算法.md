### 分词优化方案



##### ① jieba 引入外部字典

##### ② jieba 调高词频

```python
#-*- coding=utf8 -*-
import jieba
import re
# from tokenizer import cut_hanlp
# ① 引入外部词典
#jieba.load_userdict("dict.txt")
# ② 内部增加词
#jieba.add_word(row[0].strip(),tag=row[1].strip())
#jieba.suggest_freq(segment)
# ③ 调高词频
#fp=open("dict.txt",'r',encoding='utf8')
#for line in fp:
    #line=line.strip()
    #jieba.suggest_freq(line, tune=True) # 自动调高词频
from busi_analy.config.tools import cut_hanlp

[jieba.suggest_freq(line.strip(), tune=True) for line in open("../files/dict.txt",'r',encoding='utf8')]
if __name__=="__main__":
    string="测试台中是否会被切开。"

    words=' '.join(jieba.cut(string))
    words1 = cut_hanlp(string) # 调整data里面的字典
    print(words)
    print(words1)
```



##### ③ jieba 正则规则算法替换

```python
#-*- coding=utf8 -*-
import jieba
import re

from busi_analy.config.tools import cut_hanlp

# 加载外部字典
jieba.load_userdict("dict.txt")

def merge_two_list(a, b): #  合并两个列表，列表形如： ['今天 是','一天 真好 ！','不错'] ['hello','hi']
    c=[]
    len_a, len_b = len(a), len(b)
    minlen = min(len_a, len_b)
    for i in range(minlen):
        c.append(a[i])
        c.append(b[i])

    if len_a > len_b:
        for i in range(minlen, len_a):
            c.append(a[i])
    else:
        for i in range(minlen, len_b):
            c.append(b[i])  
    return c


if __name__=="__main__":
    fp=open("../files/text_nvp.txt","r",encoding="utf8")
    fout=open("../files/result_cut.txt","w",encoding="utf8")
    regex1=u'(?:[^\u4e00-\u9fa5（）*&……%￥$，,。.@! ！]){1,5}期'
    regex2=r'(?:[0-9]{1,3}[.]?[0-9]{1,3})%'
    p1=re.compile(regex1)
    p2=re.compile(regex2)
    for line in fp.readlines():
        result1=p1.findall(line)
        if result1:       
            regex_re1=result1
            line=p1.sub("FLAG1",line)
        result2=p2.findall(line)
        if result2:
            line=p2.sub("FLAG2",line)
        words=jieba.cut(line)
        words1=cut_hanlp(line)# hanlp会把FLAG1分开不适合
        result=" ".join(words)
        if "FLAG1" in result:
            result=result.split("FLAG1")
            result=merge_two_list(result,result1)
            result="".join(result)
        if "FLAG2" in result:       
            result=result.split("FLAG2")
            result=merge_two_list(result,result2)
            result="".join(result)        
        #print(result)
        fout.write(result)
    fout.close()
    
```



##### ④ 脚本程序整理字典顺序

```python
#encoding=utf8
import os
'''
整理字典字数从多到少排序
'''

# 使用os.sep是为了兼容 liunx
# dict_file=open("module"+os.sep+"hanlp"+os.sep+"data"+os.sep+"dictionary"+os.sep+"custom"+os.sep+"resume_nouns.txt",'r',encoding='utf8')
# dict_file=open(r'D:\Py_work\nlpBags\hanlp\data\dictionary\custom\CustomDictionary.txt','r',encoding='utf8')
root_path1=os.path.abspath(os.path.join(os.getcwd(), ".."+os.sep+".."))
dict_file=open(root_path1+os.sep+"models"+os.sep+"hanlp"+os.sep+"data"+os.sep+"dictionary"+os.sep+"custom"+os.sep+"CustomDictionary.txt",
               'r',encoding='utf8')
# print(dict_file)
d={}

[d.update({line:len(line.split(" ")[0])}) for line in dict_file]
print('dddd',d)
f=sorted(d.items(), key=lambda x:x[1], reverse=True)
dict_file=open(root_path1+os.sep+"models"+os.sep+"hanlp"+os.sep+"data"+os.sep+"dictionary"+os.sep+"custom"+os.sep+"CustomDictionary_1.txt"
                ,'w',encoding='utf8')
[dict_file.write(item[0]) for item in f]
dict_file.close()
```













