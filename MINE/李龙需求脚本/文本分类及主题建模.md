#### 前言

使用jupyter notebook平台进行文本数据处理

#### 1.导入数据

```python
data=pd.read_csv('./yyall.txt',header=None)
data.head()
```

使用SnowNLP训练好的模型测试得分（效果不佳，训练模型使用的是购物的数据进行训练）

```python
# 0~1 (语义积极的概率，越接近1情感表现越积极)
pro=data[0].map(lambda x:SnowNLP(x).sentiments)
pro
```

如若需要SnowNLP进行情感分析，需要训练各自领域的模型

#### 2.训练自定义的模型

​	复制路径后，Windows10默认右斜杠，把它改成左斜杠如下才可运行 训练neg.txt/pos.txt，此处使用snownlp自带的（偏购物），也可以自定义自身业务方向的词库 保存位置可自己决定，但要把`snownlp/seg/__init__.py`或`snownlp/sentiment/__init__.py`的`data_path`也改成你保存的位置，不然下次使用还是默认的。

​	用的时候，有时候可能会觉得有些语句分析出来的结果会不太准确，这时候你就需要更新语料库，再进行训练，这样下次分析出来的结果就更加准确了。

```python
# 复制路径后，Windows10默认右斜杠，把它改成左斜杠如下才可运行
# 训练neg.txt/pos.txt，此处使用snownlp自带的（偏购物），也可以自定义自身业务方向的词库
#保存位置可自己决定，但要把`snownlp/seg/__init__.py`或`snownlp/sentiment/__init__.py`的
#`data_path`也改成你保存的位置，不然下次使用还是默认的。
sentiment.train('D:/Anaconda/Lib/site-packages/snownlp/sentiment/neg.txt',
                'D:/Anaconda/Lib/site-packages/snownlp/sentiment/pos.txt')

# 本文将此保存文件移至原模型处，并修改了__init__.py
sentiment.save("./sentiment_my.marshal")
```



#### 3.引入停用词

```python
stop=pd.read_csv('./data/stop_words.utf8',sep='dsds',encoding='utf-8',header=None)
stop.head()

stop=['',' ']+list(stop[0])
stop
```



#### 4.去除停用词

```python
data['clean']=data[1].map(lambda x:[i for i in x.split() if i not in stop])
```



#### 5.LDA模型

```python
from gensim import corpora, models

# 仅仅保留2个字符以上的数据
data['c2']=data['clean'].map(lambda x: [i for i in x if len(i)>1])

# 建立模型并输出结果
data_dict = corpora.Dictionary(data['c2']) #建立词典
data_corpus = [data_dict.doc2bow(i) for i in data['c2']] #建立语料库
data_lda = models.LdaModel(data_corpus, num_topics = 5, id2word = data_dict) #LDA模型训练
for i in range(5):
    print(data_lda.print_topic(i)) #输出每个主题
```



#### 6.分类建模

引入带标签的形容词

```python
adj=pd.read_excel('./adj.xlsx')
```

导包：sklearn的文本向量转换、训练测试集划分

导包：贝叶斯——GaussianNB、MultinomialNB、BernoulliNB

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB
```

保存特征值和目标值

```python
feature=adj['adj']
target=adj['label']

X_train,x_test,y_train,y_test=train_test_split(feature,target,test_size=0.2)
```

将特征值的文本数据转换成词频模型，使用词频模型转换需要变换参与训练的文本数据

```python
#将文本转换成词频
fea_m=TfidfVectorizer().fit(feature)

#训练的词频特征数次
tf_x_train=fea_m.transform(X_train)
#测试的词频特征数据
tf_x_test=fea_m.transform(x_test)
```

使用贝叶斯模型

```python
mu=MultinomialNB()
ga=GaussianNB()
be=BernoulliNB()

mu.fit(tf_x_train,y_train)
mu.score(tf_x_test,y_test)
```



#### 7.模型持久化

```python
from sklearn.externals import joblib
joblib.dump(mu,'mu.model')
MU1=joblib.load('mu.model')
MU1.predict(s_tran)
```

举栗子：

```python
from sklearn import svm
from sklearn.externals import joblib
#训练模型
clf = svc = svm.SVC(kernel='linear')
rf=clf.fit(array(trainMat), array(listClasses))

＃保存模型
joblib.dump(rf,'rf.model')

＃加载模型
RF=joblib.load('rf.model')

＃应用模型进行预测
result=RF.predict(thsDoc)
```

#### 8.测试单句

```python
str='英美内讧越发露骨'
s=[' '.join(jieba.cut(str))]

s_tran=fea_m.transform(s)
mu.predict(s_tran)
```

#### 9.预测

处理每一行的格式成：[xx xx xx xx xx]

```python
do=data['c2'].map(lambda x:[' '.join(x)])
do
```

对每行的数据进行词向量转换，并使用分类器进行预测

```python
ll=do.map(lambda x : mu.predict(fea_m.transform(x)))
data1[1]=ll
data1[1]=data1[1].map(lambda x:x[0])
data1
```

将要预测的数据和预测的分类进行存储

```python
data1.to_excel('./all_cls.xlsx',index=None)
```



#### 10.检查核对分类并纠正，重新导入数据

```python
n_data=pd.read_excel('./all_cls.xlsx')
n_data.head()
```

导入绘图包

```python
import matplotlib.pyplot as plt
%matplotlib inline
```



#### 11.绘制饼图

```python
x1=n_data[n_data[1]==1][1].size
x_1=n_data[n_data[1]==-1][1].size

#显示中文
plt.rcParams['font.sans-serif']=['SimHei']
plt.rcParams['axes.unicode_minus']=False

colors = ['lightskyblue','orange']
plt.pie([x1,x_1], autopct='%1.2f%%',colors=colors,labels=['正面','负面'] )
plt.title('1939年-1945年')
plt.legend(('正面','负面'))

#--------------------------------
df=pd.DataFrame(data=[x1,x_1],index=['正面','负面'],columns=['1939年-1945年'])
df.plot.pie(y=0,autopct ='%1.2f%%')#饼图
plt.show()
```

![54194183067](F:\【MY_Work】\Diary\MINE\李龙需求脚本\1541941830672.png)

#### 12.LDA主题模型-正面-负面

分词并去掉停用词

```python
pos1=pos.map(lambda x:[i for i in (' '.join(jieba.cut(x))).split(' ') if i not in stop and len(i)>1])
pos1

neg1=neg.map(lambda x:[i for i in (' '.join(jieba.cut(x))).split(' ') if i not in stop and len(i)>1])
neg1
```

LDA建模

```python
# 建立模型并输出结果
pos1_dict = corpora.Dictionary(pos1) #建立词典
pos1_corpus = [pos1_dict.doc2bow(i) for i in pos1] #建立语料库
pos1_lda = models.LdaModel(pos1_corpus, num_topics = 3, id2word = pos1_dict) #LDA模型训练
for i in range(3):
    print(pos1_lda.print_topic(i)) #输出每个主题
```





