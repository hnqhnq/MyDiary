#### 词云脚本

备注：准备材料（背景图片、字体文件：使用电脑自带即可）

```python
""" 
@file: word_cloud.py
@Time: 2018/11/08
@Author:heningqiu
"""
import jieba
import numpy as np
from PIL import Image
from matplotlib import pyplot as plt
# from pyecharts import WordCloud

from wordcloud import WordCloud, STOPWORDS  # 词云
num=1939
for i in range(7):
    path='./data/yy{}.txt'.format(num)
    info=open(path,'r',encoding='utf-8',errors='ignore').read()
    cutinfo=jieba.cut(info)
    info=' '.join(cutinfo)
    # print(info)
    bgcolor=np.array(Image.open('./data/ty.jpg'))
    # print(bgcolor)
    word_Cloud=WordCloud(font_path='./data/simkai.ttf', width=800, height=800, margin=2,
                       mask=bgcolor, scale=1,max_words=200, min_font_size=4,
                     stopwords=STOPWORDS, random_state=None, background_color='white',
                     max_font_size=200, ).generate(info)
    plt.figimage(word_Cloud)
    imgpath='./img-new/yy{}.jpg'.format(num)
    plt.savefig(imgpath)
    num+=1
# plt.show()
```



#### 词频统计脚本

```python
""" 
@file: word_frequence.py
@Time: 2018/11/08
@Author:heningqiu
"""
import os, codecs
import jieba
from collections import Counter

def get_words(txt,path1):
    f=open(path1,'w',encoding='utf-8')
    seg_list = jieba.cut(txt)
    c = Counter()
    for x in seg_list:
        if len(x) > 1 and x != '\r\n':
            c[x] += 1
    print('常用词频度统计结果')
    for (k, v) in c.most_common(100):
        print('%s%s %s  %d' % ('  ' * (5 - len(k)), k, '*' * int(v / 3), v))
        # f.write( str(k)+ '---->' + str(v)+'\n')

if __name__ == '__main__':
    # num=1939
    # for i in range(7):
    #     # path='./data/yy{}.txt'.format(num)
    #     path='./word_character/p{}.txt'.format(num)
    #     with codecs.open(path, 'r', 'utf8') as f:
    #         txt = f.read()
    #     path1='./character_frequence/p{}.txt'.format(num)
    #     get_words(txt,path1)
    #     num+=1

    num="all"
    path = './data/yy{}.txt'.format(num)
    with codecs.open(path, 'r', 'utf8') as f:
        txt = f.read()
    path1 = './character_frequence/yy{}.txt'.format(num)
    get_words(txt, path1)
```



#### 名词动词介词提取脚本

```python
""" 
@file: hanlp_segcls.py
@Time: 2018/11/09
@Author:heningqiu
"""
import os
from jpype import *

root_path=os.path.abspath(os.path.join(os.getcwd(), ".."))
'''HanLP Tokenizer NLPTokenizer'''
hanlp_path=root_path+os.sep+"models"+os.sep+'hanlp'
djclass_path = r"-Djava.class.path=" + hanlp_path + os.sep + "hanlp-1.6.8.jar;" + hanlp_path # Linux需把;改成:
startJVM(getDefaultJVMPath(),djclass_path,"-Xms1g","-Xmx1g")

Tokenizer = JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer') # 标准分词
HanLP = JClass('com.hankcs.hanlp.HanLP') # hanlp分词
NLPTokenizer = JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer') # 命名实体识别和词性标注


# hanlp标准分词+词性标注
def to_string(sentence,return_generator=False):
    if return_generator:
        return (word_pos_item.toString().split('/') for word_pos_item in Tokenizer.segment(sentence)) # 形如[('新疆', 'ns'),……]
    else:
        # generator格式输出
        return [(word_pos_item.toString().split('/')[0],word_pos_item.toString().split('/')[1]) for word_pos_item in Tokenizer.segment(sentence)]

# hanlp分词+词性标注
def to_string_hanlp(sentence,return_generator=False):
    if return_generator:
        return (word_pos_item.toString().split('/') for word_pos_item in HanLP.segment(sentence)) # 形如[ ('上市公司', 'nis'),……]
    else:
        return [(word_pos_item.toString().split('/')[0],word_pos_item.toString().split('/')[1]) for word_pos_item in Tokenizer.segment(sentence)]

# 过滤空字符串以及不需要的词性字段
def seg_sentences(sentence,with_filter=True,return_generator=False):
    segs=to_string(sentence,return_generator=return_generator)
    # 过滤无用词性
    if with_filter:
        # g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair)==2 and word_pos_pair[0]!=' ' and word_pos_pair[1] not in drop_pos_set]
        g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair)==2 and word_pos_pair[0]!=' '
             and word_pos_pair[1].startswith('a')
             and len(word_pos_pair[0])>1]
    else:
        g = [(word_pos_pair[0],word_pos_pair[1]) for word_pos_pair in segs if len(word_pos_pair)==2 and word_pos_pair[0]!=' ']
    return iter(g) if return_generator else g # 仅返回词，不带词性

if __name__ == '__main__':

    # num=1939
    # for i in range(7):
    #     path1='./word_character/p{}.txt'.format(num)
    #     path2='./data/yy{}.txt'.format(num)
    #     f1=open(path1,'w',encoding='utf-8')
    #     f2=open(path2,'r',encoding='utf-8')
    #     f1.write(' '.join(seg_sentences(f2.read()))+'\n' )
    #     num+=1
    print('开始：-----')
    num='all'
    path1 = './word_character/a{}.txt'.format(num)
    path2 = './data/yy{}.txt'.format(num)
    f1 = open(path1, 'w', encoding='utf-8')
    f2 = open(path2, 'r', encoding='utf-8')
    f1.write('\n'.join(seg_sentences(f2.read())) + '\n')
    shutdownJVM()
```

